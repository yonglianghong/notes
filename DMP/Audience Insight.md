Audience Insights (AI)是一个旨在帮助页面创建者更多了解对他们重要的Facebook用户的工具。该产品将对给定人群匿名、聚合的人口统计、心理、地理和行为数据（包括Facebook本地和第三方）进行可视化。

AI需要在几百毫秒内处理数十TB数据的查询。这些查询可能包括复杂的分析计算。例如，你可能想更好地吸引18-21岁之间的美国人，了解他们对什么感兴趣。AI可以分析你的受众喜欢的Page，以及这些Page中每个人喜欢的其他Pages，从而得出一组与你的受众有高亲和力的Pages。（The AI can analyze all of your audience’s Page likes, as well as the other Pages each of these Pages has liked, in order to come up with a set of Pages that have high affinity with your audience.）在这个例子中，描述受众的前两个Pages是 "影响之上"，这是一个帮助青少年抵御负面影响和"我讨厌我的父母催促我做好准备，然后当我准备好了，他们却没有"的非营利组织。 AI往往需要通过数十亿个Page likes来计算受众的亲和力分析。

AI由一个查询引擎提供支持，查询引擎以混合整数存储的形式将数据组织在内存和闪存盘上，因此一个查询可以实时处理分布在一组机器上TB级的数据。AI查询引擎是一个有聚合器和叶子（数据）层的扇形输出的分布式系统。聚合器向所有数据节点发送查询请求，数据节点执行查询后，将本地结果发回，再进行聚合。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/aggregator-leaf.png)

AI有两种数据：(1)人口属性，如年龄、性别、国家等；(2)社交关系，如Page likes、Custom Audiences和兴趣群。这些数据集由匿名的整数组成。系统中不保留真实的用户ID。数据按用户进行分片，一个生产集群中共有1024个分片，分布在168个数据节点上。每一个分片需要将35GB的原始数据复制到数据节点上建立分片，其中内存占用8GB，闪存盘内嵌存储4GB。复制到一个集群的数据总量超过35TB，其索引在内存中共需要8TB，在嵌入式存储上共需要4TB。少量的元数据被复制到所有节点上。查询处理器访问内存中的数据比访问嵌入式存储上的数据要频繁得多。

数据有两种更新方式：一种是所有数据集的批量更新，另一种是某些索引的查询时更新。批量更新每天一次，一旦HDFS中有新的数据集，就会使用Hive流水线进行更新。每个叶子节点的后台线程从HDFS存储中复制数据，并逐一重建分片。但是，有些索引需要实时更新。叶子节点中的查询处理器在查询的同时发起获取操作，从外部数据存储中读取数据并更新shards中的索引。批量更新和查询时更新在不同的分片之间都不协调（同步），因此，集群是松散一致的，并随着时间的推移收敛到一致状态。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/leaf-node.png)

所有的属性都以倒排索引的方式进行索引。起初，我们使用传统方式进行索引，它将值映射到用户ID的列表中，如下图所示。这种索引方案需要进行交集操作来实现多个过滤器，导致大型数据集上计算量很大。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/inverted-indices.png)

为了避免交运算，我们用bitsets代替了用户IDs，用一个位表示某人是否被索引为一个属性值。现在，按位AND运算的速度比求交集快得多。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/bitset-xor.png)

另外，bitmao索引是累加的，以支持使用XOR实现的快速范围过滤，减少了范围过滤查询所需的OR操作次数。例如，需要过滤年龄在25和44之间的用户，只需要在44和24的bitmap之间进行一次XOR操作，而不是在索引不累加的情况下进行20次OR操作。需要注意的是，不管是针对一个范围的值还是单个值，都只需要一个XOR操作。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/bitmap-cumulative.png)

位图索引的内存需求与一个属性可能拥有的不同键的数量成正比。为每个键分配的位图集的大小是固定的，并不取决于键的数量。为了控制内存需求，AI有一个混合索引系统，将密集属性作为bitmap索引，将稀疏属性作为传统索引。例如，"年龄 "属性的键数只有一百多个，因此，对其进行位图索引，内存占用非常小；而 "城市 "属性有近百万个不同的键数，使用bitmap索引，需要的内存太多。稀疏属性的索引在查询执行时被转换为bitmap。虽然这需要在每次为稀疏属性运行过滤时增加一个转换步骤，但由于每个键都会生成一个稀疏bitmap，所以转换操作非常快。平均来说，bitmap索引将我们的过滤时间从50ms左右减少到5ms以下。

除了复杂的统计运算符外，查询执行中最耗时的部分一般是分组操作。对数以亿计的记录进行分组，意味着需要进行大量的整数哈希查找。受 bloom 过滤器的启发，在可能的情况下，我们使用基于bitset的过滤技术以减少逐组操作中的哈希查找次数。我们将64位整数实体ID转换为每个分片中的32位向量索引。这种转换为我们提供了连续的整数，让我们在一些统计操作中的计数操作消除了哈希。

用户属性在内存中以列式结构组织。我们首先使用集合中的对象进行组织，这让每个用户的属性在内存中的位置很接近。由于有几十个属性，这种方式在迭代数百万对象的一个具体属性时，由于跳转时间长，导致缓存效率低下。为了提高缓存效率，我们将其改为列式组织，每个属性的值都在同一个向量中，这样在迭代过程中的跳转就会小很多。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/attributes-columnar.png)

Audience Insights查询引擎实现了相当复杂的统计分析，以得出一组定义的受众具有最高亲和力的Pages。它计算出每个Pages被受众喜欢的条件概率，并从人们形成的Pages连接中发现集体兴趣，并选出最有可能喜欢的Page。这需要处理上千亿个Page点赞。AI引擎通过本地计算每个分片的候选页面，并在候选页面上进行贪婪选择，从而对亲和力计算进行扩展。

AI引擎在存在GPU硬件的情况下，可以充分利用GPU硬件来计算复杂的操作，比如亲和力计算，整个Facebook受众的计算时间高达2500ms。但是通过GPU加速，它可以在500ms以内计算亲和力，其中300ms以上在GPU上，从而节省90%的CPU时间。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/GPU-grid.png)

AI查询引擎是可扩展的。数据节点（叶子）之间不存在依赖关系，数据的移动只有一个方向——从叶子到聚合器。目前三个区域分别有168个数据节点，共计504个数据节点。所有的区域都是相同的，服务于生产流量，同时为故障转移提供冗余，因为有三个相同数据的副本。每个区域都有三台聚合机，接收前端查询请求。

由于查询引擎大量运行在内存数据上，并且有密集的统计计算，因此查询引擎是受CPU约束的。在我们目前的生产负载下，p50、p75、p95、p99的叶子查询处理时间分别为9、28、363、8600毫秒。高尾部延迟发生的原因主要有两个。(1)集群中的随机因素，如机器问题和数据集更新过程中的IO操作，使得少数节点需要花费异常的时间；(2)不同查询的查询时间之间标准差大——尤其是计算复杂的查询。

我们已经实现了尾部延迟降低技术来处理（1）。在将查询扩散到所有叶子节点后，聚合器跟踪执行时间，并检测已经响应的高延迟节点。在有足够多的叶子节点已经完成执行的情况下，它中断这些节点的执行。它的结果是处理不到100%的数据。由于人工智能查询可以容忍几个百分比的数据节点缺失，所以这种技术工作得很好，并削减了我们的尾部延迟。因此，在聚合器上报告的查询处理时间，在没有缓存的情况下，p50、p75、p95、p99分别为36、101、282、769毫秒——p99延迟降低了90%以上。使用缓存时，这些数字分别为0、32、179、446毫秒。请注意，p50为零，因为缓存，产生亚毫秒级的处理时间，服务于一半以上的查询请求。

![](https://raw.githubusercontent.com/yonglianghong/my-drive/md/img/query-execution-latencies-in-milliseconds.png)

AI查询引擎用C++实现，并使用Facebook的内部框架和库。展望未来，我们的查询引擎的下一步是支持时间序列数据集，并以一种可以在更大数量级的数据集上运行实时查询的方式组织数据。我们希望增加嵌入式存储上的数据比例，以在发展到PB级数据集时控制总内存需求。

